{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import csv, operator \n",
    "import numpy as np\n",
    "#from numpy import genfromtxt\n",
    "#from numpy import loadtxt\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# ...to serialize output arrays (bag-of-words and classes to execute supervised learning)\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargamos ahora las frases prueba en una lista\n",
    "\n",
    "Tras cargar las frases en una lista crearemos el bag-of-words asociado. \n",
    "\n",
    "Estas frases deben de llevar asoicadao su output para implementar un entrenamiento supervisado.\n",
    "\n",
    "NOTAS: \n",
    "- Quizá aqui se puedan incorporar funcionalidades, nltk, para un mejor preprocesado del texto.\n",
    "- Taambien podemos incluir esta celda y la siguiente en una funcion toGetBagOfWords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ...intentamos crear el bag of words incorporando el output que necesitaremos para el entrenamiento..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ...we generate the input file as csv, and we read it...\n",
    "\n",
    "def readFromFile(name_file, encoding, delimiter):\n",
    "    in_file = open(name_file, 'r', encoding=encoding)  # Abrir archivo csv\n",
    "    in_data = csv.reader(in_file, delimiter = delimiter)  # Leer todos los registros\n",
    "    \n",
    "    in_array = np.array([])\n",
    "    for in_register in in_data:\n",
    "        in_array = np.append(in_array, in_register)\n",
    "    in_file.close()  # Cerrar archivo\n",
    "    del in_file  # Borrar objeto\n",
    "    \n",
    "    in_array = np.reshape(in_array, (-1,2))\n",
    "    \n",
    "    return in_array\n",
    "\n",
    "#in_data = np.array(readFromFile('example-questions-supervised.csv', 'utf8', '~'))\n",
    "#print(in_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### esta lista de vectores debe de ser la entrada de la red neuronal..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createBagOfWords(corpus):\n",
    "    #print(corpus)\n",
    "    \n",
    "    vectorizer = CountVectorizer()\n",
    "    #print( vectorizer.fit_transform(corpus).todense() )\n",
    "    #print( vectorizer.vocabulary_ )\n",
    "\n",
    "    bag_of_words = vectorizer.fit_transform(corpus).todense()\n",
    "    #print(bag_of_words)\n",
    "    vocabulary = vectorizer.vocabulary_\n",
    "    \n",
    "    return vocabulary, bag_of_words\n",
    "    \n",
    "#createBagOfWords(in_data[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizeClassesOfInput(supervised_output): \n",
    "    print(supervised_output)\n",
    "    print(type(supervised_output))\n",
    "    \n",
    "    ## ...we transform output variable (categories for supervised learning) in binary-vector shape...\n",
    "    # ...firstly we trim the elements...\n",
    "    supervised_output = np.char.strip(supervised_output)\n",
    "    \n",
    "    # ...and we apply lowercase to elements...\n",
    "    supervised_output = np.char.lower(supervised_output)\n",
    "    #print(supervised_output)\n",
    "    \n",
    "    # ...and we get the unique elements...\n",
    "    unique_supervised_output = np.unique(supervised_output)\n",
    "    #print(unique_supervised_output)\n",
    "   \n",
    "    # ...we create a zeros dataframe with dimension ( length(supervised_output)xlength(unique_supervised_output) )\n",
    "    binary_supervised_output = pd.DataFrame(0, index=np.arange(len(supervised_output)), columns=unique_supervised_output)\n",
    "    #print(binary_supervised_output)\n",
    "\n",
    "    for i in range(len(supervised_output)):\n",
    "        binary_supervised_output.loc[i, supervised_output[i]] = 1\n",
    "    \n",
    "    \n",
    "    #print(binary_supervised_output)\n",
    "    \n",
    "    # ...we convert dataFrame-pandas to numpy-array\n",
    "    binary_supervised_output = binary_supervised_output.values\n",
    "    #print(binary_supervised_output)\n",
    "    \n",
    "    return binary_supervised_output\n",
    "    \n",
    "#test = np.array(['personas', 'recursos', 'recursos', 'Organizaciones', 'personas', 'recursos '])\n",
    "#vectorizeClassesOfInput(in_data[:,1])\n",
    "#vectorizeClassesOfInput(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['dime autores que saben de inteligencia artificial' 'personas']\n",
      " ['¿sabes articulos que traten de ontologias?' 'recursos']\n",
      " ['libros sobre computación cuántica' 'recursos']\n",
      " ['¿en qué universidades se estudia web semantica?' 'organizaciones']\n",
      " ['centro de investigacion en web semantica' 'organizaciones']\n",
      " ['noticias de neurociencia' 'recursos ']]\n",
      "{'autores': 2, 'articulos': 0, 'dime': 7, 'cuántica': 5, 'neurociencia': 13, 'de': 6, 'estudia': 9, 'web': 25, 'traten': 23, 'que': 16, 'semantica': 21, 'computación': 4, 'artificial': 1, 'en': 8, 'investigacion': 11, 'sobre': 22, 'noticias': 14, 'se': 20, 'qué': 17, 'saben': 18, 'centro': 3, 'inteligencia': 10, 'libros': 12, 'universidades': 24, 'sabes': 19, 'ontologias': 15}\n",
      "[[0 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 0]\n",
      " [0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0 0 1 1 0 0 1 1]\n",
      " [0 0 0 1 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1]\n",
      " [0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "['personas' 'recursos' 'recursos' 'organizaciones' 'organizaciones'\n",
      " 'recursos ']\n",
      "<class 'numpy.ndarray'>\n",
      "[[0 1 0]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "def fromSentencesToBagOfWords(name_file, encoding, delimiter):\n",
    "    \n",
    "    # read data\n",
    "    in_data = np.array(readFromFile(name_file, encoding, delimiter))\n",
    "    print(in_data)\n",
    "    \n",
    "    # create bag of words with first column in \"in_data\"\n",
    "    dic, bow = createBagOfWords(in_data[:,0])\n",
    "    print(dic)\n",
    "    print(bow)\n",
    "    \n",
    "    bow_class = vectorizeClassesOfInput(in_data[:,1]) \n",
    "    \n",
    "    print(bow_class)\n",
    "    \n",
    "    # ...to share variables...\n",
    "    #%store bow\n",
    "    #del bow\n",
    "    #%store bow_class\n",
    "    #del bow_class\n",
    "    \n",
    "    # ...to read store variables...\n",
    "    #%store -r bow_class\n",
    "    \n",
    "    # ...serializing arrays with \"pickle\" package...\n",
    "    #a = # some NumPy array\n",
    "    #bow_serialized = pickle.dump(bow, protocol=0) # protocol 0 is printable ASCII\n",
    "    #bow_class_serialized = pickle.dump(bow_class, protocol=0)\n",
    "    #deserialized_bow = pickle.loads(bow_serialized)\n",
    "    \n",
    "    with open('bows.pkl', 'wb') as objs_in:  # Python 3: open(..., 'wb')\n",
    "        pickle.dump([bow, bow_class], objs_in)\n",
    "    \n",
    "    #print(bow_serialized)\n",
    "    \n",
    "fromSentencesToBagOfWords('example-questions-supervised.csv', 'utf8', '~')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
